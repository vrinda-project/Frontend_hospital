import { useState, useRef, useEffect } from "react";

const VoiceMode = ({ sessionId, hospitalId, onClose }) => {
  const [isActive, setIsActive] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [transcript, setTranscript] = useState("");
  const [aiResponse, setAiResponse] = useState("");

  const voiceUrl = process.env.REACT_APP_VOICE;

  const wsRef = useRef(null);
  const peerConnectionRef = useRef(null);
  const localStreamRef = useRef(null);
  const audioContextRef = useRef(null);
  const voiceSessionIdRef = useRef(null);

  useEffect(() => {
    if (isActive) {
      initVoiceMode();
    }
    return () => cleanup();
  }, [isActive]);

  const initVoiceMode = async () => {
    try {
      wsRef.current = new WebSocket(`${voiceUrl}/api/v1/ws/voice-mode`);

      wsRef.current.onopen = () => {
        console.log("ğŸ“¡ WebSocket connected, sending init...");
        wsRef.current.send(
          JSON.stringify({
            type: "init",
            hospital_id: hospitalId,
          })
        );
      };

      wsRef.current.onmessage = async (event) => {
        const data = JSON.parse(event.data);
        console.log("ğŸ“¨ Received message:", data.type);

        if (data.type === "ready") {
          console.log("ğŸš€ Server ready, session_id:", data.session_id);
          voiceSessionIdRef.current = data.session_id;
          setupWebRTC();
        } else if (data.type === "answer") {
          console.log("ğŸ“¨ Received answer from server");
          const answer = new RTCSessionDescription({
            sdp: data.answer.sdp,
            type: data.answer.type
          });
          await peerConnectionRef.current.setRemoteDescription(answer);
          console.log("âœ… Handshake complete! Direct connection established");
        } else if (data.type === "ice-candidate") {
          console.log("ğŸ§Š Received ICE candidate from server");
          if (data.candidate) {
            const candidate = new RTCIceCandidate({
              sdpMid: data.candidate.sdpMid,
              sdpMLineIndex: data.candidate.sdpMLineIndex,
              candidate: data.candidate.candidate
            });
            await peerConnectionRef.current.addIceCandidate(candidate);
          }
        } else if (data.type === "transcription") {
          console.log("ğŸ“ Transcription received:", data.text);
          setTranscript(data.text);
          setIsListening(false);
        } else if (data.type === "response") {
          console.log("ğŸ’¬ AI response received:", data.text);
          setAiResponse(data.text);
          // Fix #7: Audio now comes through WebRTC, not base64
          // The backend will send audio through RTCPeerConnection
        }
      };

      wsRef.current.onerror = (error) => {
        console.error("âŒ WebSocket error:", error);
      };

      wsRef.current.onclose = () => {
        console.log("ğŸ”Œ WebSocket closed");
      };
    } catch (error) {
      console.error("âŒ Voice mode init error:", error);
    }
  };

  const setupWebRTC = async () => {
    try {
      console.log("ğŸ”„ Starting WebRTC setup...");
      
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000
        }
      });
      localStreamRef.current = stream;
      console.log("âœ… Got microphone permission with noise suppression");

      peerConnectionRef.current = new RTCPeerConnection({
        iceServers: [
          { urls: 'stun:stun.l.google.com:19302' },
          { urls: 'stun:stun1.l.google.com:19302' },
          { urls: 'stun:stun2.l.google.com:19302' }
        ]
      });
      console.log("âœ… Created peer connection");

      stream.getTracks().forEach(track => {
        peerConnectionRef.current.addTrack(track, stream);
        console.log("ğŸ™ï¸ Audio Track added");
      });
      console.log("âœ… Added microphone to connection");

      // Fix #6: Create AudioContext and store reference
      audioContextRef.current = new AudioContext();
      const analyser = audioContextRef.current.createAnalyser();
      const source = audioContextRef.current.createMediaStreamSource(stream);
      source.connect(analyser);
      
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      const checkAudioLevel = () => {
        analyser.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        if (average > 5) {
          console.log("ğŸ”Š Audio Level:", Math.round(average));
        }
        requestAnimationFrame(checkAudioLevel);
      };
      checkAudioLevel();

      peerConnectionRef.current.ontrack = (event) => {
        console.log("âœ… Received audio from server");
        const remoteAudio = new Audio();
        remoteAudio.srcObject = event.streams[0];
        remoteAudio.play();
      };

      peerConnectionRef.current.onicecandidate = (event) => {
        console.log("ğŸ§Š ICE candidate generated");
        if (event.candidate && wsRef.current?.readyState === WebSocket.OPEN) {
          console.log("ğŸ“¤ Sending ICE candidate to server");
          wsRef.current.send(JSON.stringify({
            type: "ice-candidate",
            candidate: {
              sdpMid: event.candidate.sdpMid,
              sdpMLineIndex: event.candidate.sdpMLineIndex,
              candidate: event.candidate.candidate
            }
          }));
        }
      };

      console.log("ğŸ”„ Creating offer...");
      const offer = await peerConnectionRef.current.createOffer();
      await peerConnectionRef.current.setLocalDescription(offer);
      console.log("âœ… Created offer");
      
      if (wsRef.current?.readyState === WebSocket.OPEN) {
        console.log("ğŸ“¤ Sending offer to server");
        wsRef.current.send(JSON.stringify({
          type: "offer",
          offer: {
            sdp: offer.sdp,
            type: offer.type
          }
        }));
        console.log("âœ… Sent offer to server");
      } else {
        console.error("âŒ WebSocket not ready!");
      }

      setIsListening(true);
    } catch (error) {
      console.error("âŒ WebRTC setup error:", error);
    }
  };

  const cleanup = () => {
    if (peerConnectionRef.current) {
      peerConnectionRef.current.close();
    }
    if (localStreamRef.current) {
      localStreamRef.current.getTracks().forEach((track) => track.stop());
    }
    // Fix #6: Close AudioContext to prevent memory leak
    if (audioContextRef.current) {
      audioContextRef.current.close();
    }
    if (wsRef.current) {
      wsRef.current.close();
    }
  };

  const toggleVoiceMode = () => {
    if (isActive) {
      cleanup();
      setIsActive(false);
      setIsListening(false);
      setIsSpeaking(false);
      onClose?.();
    } else {
      setIsActive(true);
    }
  };

  return (
    <div className="voice-mode-container">
      <button
        className={`voice-mode-btn ${isActive ? "active" : ""}`}
        onClick={toggleVoiceMode}
      >
        {isActive ? "ğŸ”´ Stop Voice Mode" : "ğŸ¤ Start Voice Mode"}
      </button>

      {isActive && (
        <div className="voice-mode-status">
          <div
            className={`status-indicator ${
              isListening ? "listening" : isSpeaking ? "speaking" : "idle"
            }`}
          >
            {isListening && "ğŸ¤ Listening..."}
            {isSpeaking && "ğŸ”Š AI Speaking..."}
            {!isListening && !isSpeaking && "â¸ï¸ Ready"}
          </div>

          {transcript && (
            <div className="transcript">
              <strong>You:</strong> {transcript}
            </div>
          )}

          {aiResponse && (
            <div className="ai-response">
              <strong>AI:</strong> {aiResponse}
            </div>
          )}
        </div>
      )}
    </div>
  );
};

export default VoiceMode;
